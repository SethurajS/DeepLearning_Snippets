{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text_generation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNaShjHU/cdWu6B9fKoc7bb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SethurajS/DeepLearning_Snippets/blob/master/Text_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLUuYYeu5X-D",
        "colab_type": "text"
      },
      "source": [
        "# **IMPORTING THE MODULES**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69P1sttM5YOR",
        "colab_type": "code",
        "outputId": "8ea6c754-2daf-4b37-b3cd-9b274671c3fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import time\n",
        "\n",
        "print(\"TensorFlow : {}\".format(tf.__version__))\n",
        "print(\"Numpy : {}\".format(np.__version__))\n",
        "print(\"GPU is\",\"available\" if tf.config.experimental.list_physical_devices(\"GPU\") else \"not available\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow : 2.2.0-rc3\n",
            "Numpy : 1.18.3\n",
            "GPU is available\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLDK3XSc7nM0",
        "colab_type": "text"
      },
      "source": [
        "# **IMPORTING THE DATA**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRcGUhjD7neh",
        "colab_type": "code",
        "outputId": "01259fbd-6c98-4367-eff5-7ba0a14f2ba9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "file_path = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1122304/1115394 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_i4P1h176Ub",
        "colab_type": "code",
        "outputId": "e7f704da-d2a7-47b2-d477-0d1d69f1f46e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        }
      },
      "source": [
        "text = open(file_path, 'rb').read().decode(encoding='utf-8')\n",
        "\n",
        "print(\"First 250 chars : ----------> \\n\\n{}\".format(text[:250]),)\n",
        "print(\"Length of the text : ----------> {}\".format(len(text)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First 250 chars : ----------> \n",
            "\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "Length of the text : ----------> 1115394\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hupyYbkv8Qrl",
        "colab_type": "text"
      },
      "source": [
        "# **PREPROCESSING THE DATA**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UoqHI2TPELLi",
        "colab_type": "text"
      },
      "source": [
        "**FETCHING UNIQUE CHARACTERS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgdNQooa84YS",
        "colab_type": "code",
        "outputId": "bec63ea3-74e2-4fea-e8c5-f137ba3cd0ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "vocab = sorted(set(text))\n",
        "print(\"Vocab List : {}\".format(vocab))\n",
        "print(\"Vocab Length : {}\".format(len(vocab)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocab List : ['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
            "Vocab Length : 65\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0wT6W9WEWRL",
        "colab_type": "text"
      },
      "source": [
        "**DATA CONVERTION**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4IkD1jF9UBy",
        "colab_type": "code",
        "outputId": "71adb6be-629a-405d-9ff0-760faa9f0830",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "char_to_idx = {char: index for index, char in enumerate(vocab)}\n",
        "idx_to_char = np.array(vocab)\n",
        "\n",
        "print(\"Char --> Index dict : {}\".format(char_to_idx))\n",
        "print(\"Index --> Char array : {}\".format(idx_to_char))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Char --> Index dict : {'\\n': 0, ' ': 1, '!': 2, '$': 3, '&': 4, \"'\": 5, ',': 6, '-': 7, '.': 8, '3': 9, ':': 10, ';': 11, '?': 12, 'A': 13, 'B': 14, 'C': 15, 'D': 16, 'E': 17, 'F': 18, 'G': 19, 'H': 20, 'I': 21, 'J': 22, 'K': 23, 'L': 24, 'M': 25, 'N': 26, 'O': 27, 'P': 28, 'Q': 29, 'R': 30, 'S': 31, 'T': 32, 'U': 33, 'V': 34, 'W': 35, 'X': 36, 'Y': 37, 'Z': 38, 'a': 39, 'b': 40, 'c': 41, 'd': 42, 'e': 43, 'f': 44, 'g': 45, 'h': 46, 'i': 47, 'j': 48, 'k': 49, 'l': 50, 'm': 51, 'n': 52, 'o': 53, 'p': 54, 'q': 55, 'r': 56, 's': 57, 't': 58, 'u': 59, 'v': 60, 'w': 61, 'x': 62, 'y': 63, 'z': 64}\n",
            "Index --> Char array : ['\\n' ' ' '!' '$' '&' \"'\" ',' '-' '.' '3' ':' ';' '?' 'A' 'B' 'C' 'D' 'E'\n",
            " 'F' 'G' 'H' 'I' 'J' 'K' 'L' 'M' 'N' 'O' 'P' 'Q' 'R' 'S' 'T' 'U' 'V' 'W'\n",
            " 'X' 'Y' 'Z' 'a' 'b' 'c' 'd' 'e' 'f' 'g' 'h' 'i' 'j' 'k' 'l' 'm' 'n' 'o'\n",
            " 'p' 'q' 'r' 's' 't' 'u' 'v' 'w' 'x' 'y' 'z']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75JtCovYEdD4",
        "colab_type": "text"
      },
      "source": [
        "**LOOKING INTO THE DATA AFTER CONVERSION**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2Bhnwqn-YVy",
        "colab_type": "code",
        "outputId": "38cac250-6218-4908-d2b9-bdcd8ad563c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        }
      },
      "source": [
        "text_as_int = [char_to_idx[c] for c in text]\n",
        "\n",
        "print(\"Actual text : -------------> \\n\\n{}\".format(text[:250]))\n",
        "print(\"Converted text : -------------> \\n\\n{}\".format(text_as_int[:250]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Actual text : -------------> \n",
            "\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "Converted text : -------------> \n",
            "\n",
            "[18, 47, 56, 57, 58, 1, 15, 47, 58, 47, 64, 43, 52, 10, 0, 14, 43, 44, 53, 56, 43, 1, 61, 43, 1, 54, 56, 53, 41, 43, 43, 42, 1, 39, 52, 63, 1, 44, 59, 56, 58, 46, 43, 56, 6, 1, 46, 43, 39, 56, 1, 51, 43, 1, 57, 54, 43, 39, 49, 8, 0, 0, 13, 50, 50, 10, 0, 31, 54, 43, 39, 49, 6, 1, 57, 54, 43, 39, 49, 8, 0, 0, 18, 47, 56, 57, 58, 1, 15, 47, 58, 47, 64, 43, 52, 10, 0, 37, 53, 59, 1, 39, 56, 43, 1, 39, 50, 50, 1, 56, 43, 57, 53, 50, 60, 43, 42, 1, 56, 39, 58, 46, 43, 56, 1, 58, 53, 1, 42, 47, 43, 1, 58, 46, 39, 52, 1, 58, 53, 1, 44, 39, 51, 47, 57, 46, 12, 0, 0, 13, 50, 50, 10, 0, 30, 43, 57, 53, 50, 60, 43, 42, 8, 1, 56, 43, 57, 53, 50, 60, 43, 42, 8, 0, 0, 18, 47, 56, 57, 58, 1, 15, 47, 58, 47, 64, 43, 52, 10, 0, 18, 47, 56, 57, 58, 6, 1, 63, 53, 59, 1, 49, 52, 53, 61, 1, 15, 39, 47, 59, 57, 1, 25, 39, 56, 41, 47, 59, 57, 1, 47, 57, 1, 41, 46, 47, 43, 44, 1, 43, 52, 43, 51, 63, 1, 58, 53, 1, 58, 46, 43, 1, 54, 43, 53, 54, 50, 43, 8, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfL9Hqp4EnLG",
        "colab_type": "text"
      },
      "source": [
        "**CONVERTING THE DATA INTO TF DATASETS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dceuyxk3AOzk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "datasets = tf.data.Dataset.from_tensor_slices(text_as_int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcZSlDpJExCB",
        "colab_type": "text"
      },
      "source": [
        "**CREATING INPUT SEQUENCE OF LENGTH - (100)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILzmTeYEAlwh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sequence_length = 100\n",
        "sequence = datasets.batch(sequence_length+1, drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbi25OETFKUv",
        "colab_type": "text"
      },
      "source": [
        "**SPLITING THE SEQUNECES INTO INPUT AND OUTPUT DATA**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXKBqt0CA8BI",
        "colab_type": "code",
        "outputId": "22aa6e0b-4348-4941-b10b-d93b31e54e52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        }
      },
      "source": [
        "def input_output(data):\n",
        "  input_data = data[:-1]\n",
        "  output_data = data[1:]\n",
        "  return input_data, output_data\n",
        "\n",
        "data = sequence.map(input_output)\n",
        "\n",
        "for inputs, outputs in data.take(1):\n",
        "  print(\"Input data : ------> \\n\\n{}\".format(inputs), end=\"\\n\\n\")\n",
        "  print(\"Input data text format : ------> \\n\\n{}\".format(repr(''.join(idx_to_char[inputs.numpy()]))), end=\"\\n\\n\\n\\n\")\n",
        "  print(\"Output data : ------> \\n\\n{}\".format(outputs), end=\"\\n\\n\")\n",
        "  print(\"Output data text format : ------> \\n\\n{}\".format(repr(''.join(idx_to_char[outputs.numpy()]))))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input data : ------> \n",
            "\n",
            "[18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 14 43 44 53 56 43  1 61 43\n",
            "  1 54 56 53 41 43 43 42  1 39 52 63  1 44 59 56 58 46 43 56  6  1 46 43\n",
            " 39 56  1 51 43  1 57 54 43 39 49  8  0  0 13 50 50 10  0 31 54 43 39 49\n",
            "  6  1 57 54 43 39 49  8  0  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10\n",
            "  0 37 53 59]\n",
            "\n",
            "Input data text format : ------> \n",
            "\n",
            "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "\n",
            "\n",
            "\n",
            "Output data : ------> \n",
            "\n",
            "[47 56 57 58  1 15 47 58 47 64 43 52 10  0 14 43 44 53 56 43  1 61 43  1\n",
            " 54 56 53 41 43 43 42  1 39 52 63  1 44 59 56 58 46 43 56  6  1 46 43 39\n",
            " 56  1 51 43  1 57 54 43 39 49  8  0  0 13 50 50 10  0 31 54 43 39 49  6\n",
            "  1 57 54 43 39 49  8  0  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10  0\n",
            " 37 53 59  1]\n",
            "\n",
            "Output data text format : ------> \n",
            "\n",
            "'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMk1w8foFkIo",
        "colab_type": "text"
      },
      "source": [
        "## **CREATING THE TRAINING BATCHES**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pOgus8wFwq0",
        "colab_type": "code",
        "outputId": "255245f2-3ca3-4ea7-e965-c15de72ee56a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "SHUFFLE_SIZE = 10000\n",
        "\n",
        "dataset = data.shuffle(SHUFFLE_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "dataset"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int32, tf.int32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycvQKJzUGbVD",
        "colab_type": "text"
      },
      "source": [
        "# **BUILDING THE MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1OZxX_6H1bz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VOCAB_SIZE = len(vocab)\n",
        "EMBEDDING_DIM = 256\n",
        "RNN_UNITS = 1024\n",
        "BATCH_SIZE = 64"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwuZbjTaGiH4",
        "colab_type": "code",
        "outputId": "01b80e8e-60e4-4cab-c998-3db4d2835b28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        }
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential()\n",
        "  model.add(tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]))\n",
        "  model.add(tf.keras.layers.LSTM(rnn_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'))\n",
        "  model.add(tf.keras.layers.Dense(vocab_size))\n",
        "\n",
        "  return model\n",
        "\n",
        "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, BATCH_SIZE)\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (64, None, 256)           16640     \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (64, None, 1024)          5246976   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (64, None, 65)            66625     \n",
            "=================================================================\n",
            "Total params: 5,330,241\n",
            "Trainable params: 5,330,241\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ef9WBjyHIdmC",
        "colab_type": "text"
      },
      "source": [
        "**TRYING MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAgEf-6BIbrq",
        "colab_type": "code",
        "outputId": "6cae2c5b-6d44-4b85-9c2f-440c53efbabb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "for input_data, output_data in dataset.take(1):\n",
        "  print(\"Input_data shape : {}\".format((model(input_data)).shape))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input_data shape : (64, 100, 65)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIn_lMHQKuJW",
        "colab_type": "text"
      },
      "source": [
        "# **COMPILING THE MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MglWutdkKwvR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KRry6zWLcir",
        "colab_type": "text"
      },
      "source": [
        "## **CONFIGURING CHECKPOINTS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmCjFc5aLndx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt_{epoch}')\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix, save_weights_only=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3YweR5aMfVS",
        "colab_type": "text"
      },
      "source": [
        "# **TRAINING THE MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xASf7QHIMje4",
        "colab_type": "code",
        "outputId": "d6477e53-16ac-4cc7-d804-3cda35e8f576",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "EPOCHS = 50\n",
        "\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "172/172 [==============================] - 9s 55ms/step - loss: 2.5540\n",
            "Epoch 2/50\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 1.8641\n",
            "Epoch 3/50\n",
            "172/172 [==============================] - 9s 55ms/step - loss: 1.6208\n",
            "Epoch 4/50\n",
            "172/172 [==============================] - 9s 55ms/step - loss: 1.4928\n",
            "Epoch 5/50\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 1.4145\n",
            "Epoch 6/50\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 1.3585\n",
            "Epoch 7/50\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 1.3147\n",
            "Epoch 8/50\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 1.2757\n",
            "Epoch 9/50\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 1.2387\n",
            "Epoch 10/50\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 1.2031\n",
            "Epoch 11/50\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 1.1662\n",
            "Epoch 12/50\n",
            "172/172 [==============================] - 9s 55ms/step - loss: 1.1292\n",
            "Epoch 13/50\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 1.0911\n",
            "Epoch 14/50\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 1.0507\n",
            "Epoch 15/50\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 1.0101\n",
            "Epoch 16/50\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.9686\n",
            "Epoch 17/50\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.9271\n",
            "Epoch 18/50\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.8848\n",
            "Epoch 19/50\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.8449\n",
            "Epoch 20/50\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.8068\n",
            "Epoch 21/50\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.7711\n",
            "Epoch 22/50\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.7372\n",
            "Epoch 23/50\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.7067\n",
            "Epoch 24/50\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.6778\n",
            "Epoch 25/50\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.6518\n",
            "Epoch 26/50\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.6284\n",
            "Epoch 27/50\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.6076\n",
            "Epoch 28/50\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.5881\n",
            "Epoch 29/50\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.5707\n",
            "Epoch 30/50\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.5542\n",
            "Epoch 31/50\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.5401\n",
            "Epoch 32/50\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.5272\n",
            "Epoch 33/50\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.5165\n",
            "Epoch 34/50\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.5078\n",
            "Epoch 35/50\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.4966\n",
            "Epoch 36/50\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.4888\n",
            "Epoch 37/50\n",
            "172/172 [==============================] - 9s 53ms/step - loss: 0.4800\n",
            "Epoch 38/50\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.4743\n",
            "Epoch 39/50\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.4683\n",
            "Epoch 40/50\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.4636\n",
            "Epoch 41/50\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.4584\n",
            "Epoch 42/50\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.4536\n",
            "Epoch 43/50\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.4489\n",
            "Epoch 44/50\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.4434\n",
            "Epoch 45/50\n",
            "172/172 [==============================] - 9s 53ms/step - loss: 0.4404\n",
            "Epoch 46/50\n",
            "172/172 [==============================] - 9s 53ms/step - loss: 0.4370\n",
            "Epoch 47/50\n",
            "172/172 [==============================] - 9s 53ms/step - loss: 0.4332\n",
            "Epoch 48/50\n",
            "172/172 [==============================] - 9s 53ms/step - loss: 0.4310\n",
            "Epoch 49/50\n",
            "172/172 [==============================] - 9s 53ms/step - loss: 0.4294\n",
            "Epoch 50/50\n",
            "172/172 [==============================] - 9s 53ms/step - loss: 0.4252\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-O3ybpknObLc",
        "colab_type": "text"
      },
      "source": [
        "# **GENERATING TEXT**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Le0vFFt_OfpE",
        "colab_type": "text"
      },
      "source": [
        "**RESTORING THE LATEST CHECKPOINTS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSbGIG-EOeYx",
        "colab_type": "code",
        "outputId": "018419c3-7e99-4416-9551-e33a158704ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        }
      },
      "source": [
        "# tf.train.latest_checkpoint(checkpoint_dir)\n",
        "\n",
        "#model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, batch_size=1)\n",
        "\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "model.build(tf.TensorShape([1, None]))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (1, None, 256)            16640     \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (1, None, 1024)           5246976   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (1, None, 65)             66625     \n",
            "=================================================================\n",
            "Total params: 5,330,241\n",
            "Trainable params: 5,330,241\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxCtkAYHQtBx",
        "colab_type": "text"
      },
      "source": [
        "**TEXT GENERATION**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPu8Y0F9QylP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "\n",
        "  num_generate = 1000\n",
        "\n",
        "  input_eval = [char_to_idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  text_generated = []\n",
        "\n",
        "  temp = 1.0\n",
        "\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "    \n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "      predictions = predictions / temp\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      text_generated.append(idx_to_char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1T2bICGYVN2x",
        "colab_type": "code",
        "outputId": "a5ae543e-089e-43cc-c802-6812cba59150",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 554
        }
      },
      "source": [
        "inp = input(u\"Type a starting string: \\n\\n\")\n",
        "print(generate_text(model, inp))\n",
        "\n",
        "# print(generate_text(model, start_string=u\"ROMEO: \"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Type a starting string: \n",
            "\n",
            "romeo\n",
            "romeones not found? O that may let it friend, I cry thee banishment: I receive my\n",
            "natious youngest-day never cut for Claudio:\n",
            "Ba taste the streets, and so storm\n",
            "That we have broken shows a wife to thee.\n",
            "\n",
            "QUEEN ELIZABETH:\n",
            "Oh, who shall poison here,\n",
            "Whose arms and soldiers, and the thorn be seen as I can\n",
            "learnedge to this proud we'll nor slarpestagener, like my breast!\n",
            "How nearly will command thee say that I have King Lewns,\n",
            "Hath he keposs he may live: and so do I will abservant villain!\n",
            "Well, dead my marriage!\n",
            "\n",
            "Both Tribunes, which strike upon my father's laid,\n",
            "Whose\n",
            "' would be ready with her to imprison't be done,\n",
            "Enforceal summons of our commonwealth\n",
            "'Gainst fair Exellikence honour of a pleasure;\n",
            "Thy slaughter'd birth, you'll me, ghands:\n",
            "Hold huntsmine honour to loving spring;\n",
            "But sees you think the worst can curds she will be oat.\n",
            "\n",
            "ROMEO:\n",
            "Did they be satisfied. Give me a travel,\n",
            "Till he had gone at your power to do thee this place.\n",
            "\n",
            "CLARENCE:\n",
            "My LADY VINCENTIO:\n",
            "Come on, my lord and fear i\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4iMLstxg8cu",
        "colab_type": "text"
      },
      "source": [
        "# **CUSTOMIZED MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCsTvS43hAqe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qiXvDHeEh7tQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6oIqsBIiS4x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train(inp, target):\n",
        "  with tf.GradientTape() as tape:\n",
        "    prediction = model(inp)\n",
        "    loss = tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(target, prediction, from_logits=True))\n",
        "  grad = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(grad, model.trainable_variables))\n",
        "  \n",
        "  return loss\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmKtOP-Cj3SY",
        "colab_type": "code",
        "outputId": "973b23b5-b1c0-4ac4-90a3-0880e5b238cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "EPOCHS = 50\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  hidden = model.reset_states()\n",
        "\n",
        "  for batch, (inp, target) in enumerate(dataset):\n",
        "\n",
        "    loss = train(inp, target)\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      print(\"Epochs {} -- Batchs {} -- Loss {}\".format(epoch+1, batch, loss))\n",
        "\n",
        "  if (epoch+1) % 5 == 0:\n",
        "    model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
        "\n",
        "  print(\"Epochs {} -- Loss {}\".format(epoch+1, loss))\n",
        "  print(\"Time for epoch {} -- {} sec\\n\".format(epoch+1, time.time() - start))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epochs 1 -- Batchs 0 -- Loss 4.175083160400391\n",
            "Epochs 1 -- Batchs 100 -- Loss 2.3756961822509766\n",
            "Epochs 1 -- Loss 2.121181011199951\n",
            "Time for epoch 1 -- 10.38376522064209 sec\n",
            "\n",
            "Epochs 2 -- Batchs 0 -- Loss 2.601860284805298\n",
            "Epochs 2 -- Batchs 100 -- Loss 1.949493408203125\n",
            "Epochs 2 -- Loss 1.8026353120803833\n",
            "Time for epoch 2 -- 9.266782760620117 sec\n",
            "\n",
            "Epochs 3 -- Batchs 0 -- Loss 1.78058660030365\n",
            "Epochs 3 -- Batchs 100 -- Loss 1.6772582530975342\n",
            "Epochs 3 -- Loss 1.6218703985214233\n",
            "Time for epoch 3 -- 9.30790376663208 sec\n",
            "\n",
            "Epochs 4 -- Batchs 0 -- Loss 1.5847848653793335\n",
            "Epochs 4 -- Batchs 100 -- Loss 1.5768003463745117\n",
            "Epochs 4 -- Loss 1.510960578918457\n",
            "Time for epoch 4 -- 9.38113808631897 sec\n",
            "\n",
            "Epochs 5 -- Batchs 0 -- Loss 1.4702422618865967\n",
            "Epochs 5 -- Batchs 100 -- Loss 1.4624484777450562\n",
            "Epochs 5 -- Loss 1.3865175247192383\n",
            "Time for epoch 5 -- 9.364110469818115 sec\n",
            "\n",
            "Epochs 6 -- Batchs 0 -- Loss 1.3845782279968262\n",
            "Epochs 6 -- Batchs 100 -- Loss 1.397548794746399\n",
            "Epochs 6 -- Loss 1.334306240081787\n",
            "Time for epoch 6 -- 9.30226755142212 sec\n",
            "\n",
            "Epochs 7 -- Batchs 0 -- Loss 1.4132722616195679\n",
            "Epochs 7 -- Batchs 100 -- Loss 1.3303433656692505\n",
            "Epochs 7 -- Loss 1.365648627281189\n",
            "Time for epoch 7 -- 9.264132738113403 sec\n",
            "\n",
            "Epochs 8 -- Batchs 0 -- Loss 1.3231897354125977\n",
            "Epochs 8 -- Batchs 100 -- Loss 1.3421814441680908\n",
            "Epochs 8 -- Loss 1.3119012117385864\n",
            "Time for epoch 8 -- 9.2526376247406 sec\n",
            "\n",
            "Epochs 9 -- Batchs 0 -- Loss 1.288301706314087\n",
            "Epochs 9 -- Batchs 100 -- Loss 1.3284540176391602\n",
            "Epochs 9 -- Loss 1.2961310148239136\n",
            "Time for epoch 9 -- 9.283647060394287 sec\n",
            "\n",
            "Epochs 10 -- Batchs 0 -- Loss 1.24518620967865\n",
            "Epochs 10 -- Batchs 100 -- Loss 1.2793482542037964\n",
            "Epochs 10 -- Loss 1.253702998161316\n",
            "Time for epoch 10 -- 9.35328221321106 sec\n",
            "\n",
            "Epochs 11 -- Batchs 0 -- Loss 1.2473427057266235\n",
            "Epochs 11 -- Batchs 100 -- Loss 1.2500412464141846\n",
            "Epochs 11 -- Loss 1.2752704620361328\n",
            "Time for epoch 11 -- 9.31272029876709 sec\n",
            "\n",
            "Epochs 12 -- Batchs 0 -- Loss 1.1897538900375366\n",
            "Epochs 12 -- Batchs 100 -- Loss 1.2380274534225464\n",
            "Epochs 12 -- Loss 1.188875436782837\n",
            "Time for epoch 12 -- 9.279345989227295 sec\n",
            "\n",
            "Epochs 13 -- Batchs 0 -- Loss 1.1500351428985596\n",
            "Epochs 13 -- Batchs 100 -- Loss 1.2084954977035522\n",
            "Epochs 13 -- Loss 1.165705680847168\n",
            "Time for epoch 13 -- 9.293696880340576 sec\n",
            "\n",
            "Epochs 14 -- Batchs 0 -- Loss 1.1266436576843262\n",
            "Epochs 14 -- Batchs 100 -- Loss 1.1395037174224854\n",
            "Epochs 14 -- Loss 1.178380012512207\n",
            "Time for epoch 14 -- 9.284358978271484 sec\n",
            "\n",
            "Epochs 15 -- Batchs 0 -- Loss 1.0854099988937378\n",
            "Epochs 15 -- Batchs 100 -- Loss 1.1768851280212402\n",
            "Epochs 15 -- Loss 1.1370514631271362\n",
            "Time for epoch 15 -- 9.303475141525269 sec\n",
            "\n",
            "Epochs 16 -- Batchs 0 -- Loss 1.0401960611343384\n",
            "Epochs 16 -- Batchs 100 -- Loss 1.1082963943481445\n",
            "Epochs 16 -- Loss 1.1192078590393066\n",
            "Time for epoch 16 -- 9.300780773162842 sec\n",
            "\n",
            "Epochs 17 -- Batchs 0 -- Loss 1.015464425086975\n",
            "Epochs 17 -- Batchs 100 -- Loss 1.0797233581542969\n",
            "Epochs 17 -- Loss 1.0898921489715576\n",
            "Time for epoch 17 -- 9.283455848693848 sec\n",
            "\n",
            "Epochs 18 -- Batchs 0 -- Loss 0.9677804708480835\n",
            "Epochs 18 -- Batchs 100 -- Loss 1.0714898109436035\n",
            "Epochs 18 -- Loss 1.0753300189971924\n",
            "Time for epoch 18 -- 9.300550937652588 sec\n",
            "\n",
            "Epochs 19 -- Batchs 0 -- Loss 0.9806067943572998\n",
            "Epochs 19 -- Batchs 100 -- Loss 1.059929370880127\n",
            "Epochs 19 -- Loss 1.020243525505066\n",
            "Time for epoch 19 -- 9.28884744644165 sec\n",
            "\n",
            "Epochs 20 -- Batchs 0 -- Loss 0.9111466407775879\n",
            "Epochs 20 -- Batchs 100 -- Loss 0.9996547698974609\n",
            "Epochs 20 -- Loss 1.0159586668014526\n",
            "Time for epoch 20 -- 9.332661628723145 sec\n",
            "\n",
            "Epochs 21 -- Batchs 0 -- Loss 0.9051415920257568\n",
            "Epochs 21 -- Batchs 100 -- Loss 0.9870316982269287\n",
            "Epochs 21 -- Loss 0.9873466491699219\n",
            "Time for epoch 21 -- 9.34220814704895 sec\n",
            "\n",
            "Epochs 22 -- Batchs 0 -- Loss 0.8676663041114807\n",
            "Epochs 22 -- Batchs 100 -- Loss 0.9228129386901855\n",
            "Epochs 22 -- Loss 0.9376100897789001\n",
            "Time for epoch 22 -- 9.29177737236023 sec\n",
            "\n",
            "Epochs 23 -- Batchs 0 -- Loss 0.8247313499450684\n",
            "Epochs 23 -- Batchs 100 -- Loss 0.8977269530296326\n",
            "Epochs 23 -- Loss 0.9081158638000488\n",
            "Time for epoch 23 -- 9.284063577651978 sec\n",
            "\n",
            "Epochs 24 -- Batchs 0 -- Loss 0.8009180426597595\n",
            "Epochs 24 -- Batchs 100 -- Loss 0.892744779586792\n",
            "Epochs 24 -- Loss 0.8933166265487671\n",
            "Time for epoch 24 -- 9.297922849655151 sec\n",
            "\n",
            "Epochs 25 -- Batchs 0 -- Loss 0.7519614696502686\n",
            "Epochs 25 -- Batchs 100 -- Loss 0.8381180763244629\n",
            "Epochs 25 -- Loss 0.8567376732826233\n",
            "Time for epoch 25 -- 9.334155321121216 sec\n",
            "\n",
            "Epochs 26 -- Batchs 0 -- Loss 0.7317611575126648\n",
            "Epochs 26 -- Batchs 100 -- Loss 0.8256202936172485\n",
            "Epochs 26 -- Loss 0.8125945925712585\n",
            "Time for epoch 26 -- 9.312259435653687 sec\n",
            "\n",
            "Epochs 27 -- Batchs 0 -- Loss 0.6939055919647217\n",
            "Epochs 27 -- Batchs 100 -- Loss 0.7826568484306335\n",
            "Epochs 27 -- Loss 0.8312743306159973\n",
            "Time for epoch 27 -- 9.266303300857544 sec\n",
            "\n",
            "Epochs 28 -- Batchs 0 -- Loss 0.6860885620117188\n",
            "Epochs 28 -- Batchs 100 -- Loss 0.7585140466690063\n",
            "Epochs 28 -- Loss 0.7716142535209656\n",
            "Time for epoch 28 -- 9.278655529022217 sec\n",
            "\n",
            "Epochs 29 -- Batchs 0 -- Loss 0.6490991115570068\n",
            "Epochs 29 -- Batchs 100 -- Loss 0.7267603278160095\n",
            "Epochs 29 -- Loss 0.7637647986412048\n",
            "Time for epoch 29 -- 9.295234203338623 sec\n",
            "\n",
            "Epochs 30 -- Batchs 0 -- Loss 0.6115472912788391\n",
            "Epochs 30 -- Batchs 100 -- Loss 0.7320210337638855\n",
            "Epochs 30 -- Loss 0.7200169563293457\n",
            "Time for epoch 30 -- 9.356533527374268 sec\n",
            "\n",
            "Epochs 31 -- Batchs 0 -- Loss 0.5860301852226257\n",
            "Epochs 31 -- Batchs 100 -- Loss 0.6713740825653076\n",
            "Epochs 31 -- Loss 0.7021973133087158\n",
            "Time for epoch 31 -- 9.304218769073486 sec\n",
            "\n",
            "Epochs 32 -- Batchs 0 -- Loss 0.5792833566665649\n",
            "Epochs 32 -- Batchs 100 -- Loss 0.6759650707244873\n",
            "Epochs 32 -- Loss 0.7009083032608032\n",
            "Time for epoch 32 -- 9.259823560714722 sec\n",
            "\n",
            "Epochs 33 -- Batchs 0 -- Loss 0.5468850135803223\n",
            "Epochs 33 -- Batchs 100 -- Loss 0.6507139801979065\n",
            "Epochs 33 -- Loss 0.6609610915184021\n",
            "Time for epoch 33 -- 9.288352727890015 sec\n",
            "\n",
            "Epochs 34 -- Batchs 0 -- Loss 0.5686381459236145\n",
            "Epochs 34 -- Batchs 100 -- Loss 0.6025944352149963\n",
            "Epochs 34 -- Loss 0.6355816721916199\n",
            "Time for epoch 34 -- 9.25933313369751 sec\n",
            "\n",
            "Epochs 35 -- Batchs 0 -- Loss 0.5083196759223938\n",
            "Epochs 35 -- Batchs 100 -- Loss 0.6376562714576721\n",
            "Epochs 35 -- Loss 0.6504574418067932\n",
            "Time for epoch 35 -- 9.318222999572754 sec\n",
            "\n",
            "Epochs 36 -- Batchs 0 -- Loss 0.5067471861839294\n",
            "Epochs 36 -- Batchs 100 -- Loss 0.6037500500679016\n",
            "Epochs 36 -- Loss 0.6332922577857971\n",
            "Time for epoch 36 -- 9.279820442199707 sec\n",
            "\n",
            "Epochs 37 -- Batchs 0 -- Loss 0.4938923716545105\n",
            "Epochs 37 -- Batchs 100 -- Loss 0.5739741325378418\n",
            "Epochs 37 -- Loss 0.6235944628715515\n",
            "Time for epoch 37 -- 9.299254655838013 sec\n",
            "\n",
            "Epochs 38 -- Batchs 0 -- Loss 0.4641261398792267\n",
            "Epochs 38 -- Batchs 100 -- Loss 0.5705199241638184\n",
            "Epochs 38 -- Loss 0.6095531582832336\n",
            "Time for epoch 38 -- 9.352718353271484 sec\n",
            "\n",
            "Epochs 39 -- Batchs 0 -- Loss 0.44893983006477356\n",
            "Epochs 39 -- Batchs 100 -- Loss 0.5694290995597839\n",
            "Epochs 39 -- Loss 0.5802592039108276\n",
            "Time for epoch 39 -- 9.302011728286743 sec\n",
            "\n",
            "Epochs 40 -- Batchs 0 -- Loss 0.4308583736419678\n",
            "Epochs 40 -- Batchs 100 -- Loss 0.5503532290458679\n",
            "Epochs 40 -- Loss 0.568259060382843\n",
            "Time for epoch 40 -- 9.307921648025513 sec\n",
            "\n",
            "Epochs 41 -- Batchs 0 -- Loss 0.4424346685409546\n",
            "Epochs 41 -- Batchs 100 -- Loss 0.5337259769439697\n",
            "Epochs 41 -- Loss 0.5516919493675232\n",
            "Time for epoch 41 -- 9.332521200180054 sec\n",
            "\n",
            "Epochs 42 -- Batchs 0 -- Loss 0.42244383692741394\n",
            "Epochs 42 -- Batchs 100 -- Loss 0.537092387676239\n",
            "Epochs 42 -- Loss 0.5548785328865051\n",
            "Time for epoch 42 -- 9.264395236968994 sec\n",
            "\n",
            "Epochs 43 -- Batchs 0 -- Loss 0.41851845383644104\n",
            "Epochs 43 -- Batchs 100 -- Loss 0.5208063125610352\n",
            "Epochs 43 -- Loss 0.5545657277107239\n",
            "Time for epoch 43 -- 9.279571294784546 sec\n",
            "\n",
            "Epochs 44 -- Batchs 0 -- Loss 0.39557212591171265\n",
            "Epochs 44 -- Batchs 100 -- Loss 0.5005972981452942\n",
            "Epochs 44 -- Loss 0.5428375005722046\n",
            "Time for epoch 44 -- 9.268662929534912 sec\n",
            "\n",
            "Epochs 45 -- Batchs 0 -- Loss 0.3977106809616089\n",
            "Epochs 45 -- Batchs 100 -- Loss 0.5226589441299438\n",
            "Epochs 45 -- Loss 0.5267833471298218\n",
            "Time for epoch 45 -- 9.349648237228394 sec\n",
            "\n",
            "Epochs 46 -- Batchs 0 -- Loss 0.4060226380825043\n",
            "Epochs 46 -- Batchs 100 -- Loss 0.49351537227630615\n",
            "Epochs 46 -- Loss 0.518074095249176\n",
            "Time for epoch 46 -- 9.318261623382568 sec\n",
            "\n",
            "Epochs 47 -- Batchs 0 -- Loss 0.39223605394363403\n",
            "Epochs 47 -- Batchs 100 -- Loss 0.48963600397109985\n",
            "Epochs 47 -- Loss 0.5253840088844299\n",
            "Time for epoch 47 -- 9.270821809768677 sec\n",
            "\n",
            "Epochs 48 -- Batchs 0 -- Loss 0.3857213854789734\n",
            "Epochs 48 -- Batchs 100 -- Loss 0.4890778064727783\n",
            "Epochs 48 -- Loss 0.5016451478004456\n",
            "Time for epoch 48 -- 9.290894508361816 sec\n",
            "\n",
            "Epochs 49 -- Batchs 0 -- Loss 0.37597522139549255\n",
            "Epochs 49 -- Batchs 100 -- Loss 0.4895011782646179\n",
            "Epochs 49 -- Loss 0.5277101397514343\n",
            "Time for epoch 49 -- 9.28806185722351 sec\n",
            "\n",
            "Epochs 50 -- Batchs 0 -- Loss 0.3690911829471588\n",
            "Epochs 50 -- Batchs 100 -- Loss 0.4979853928089142\n",
            "Epochs 50 -- Loss 0.49452918767929077\n",
            "Time for epoch 50 -- 9.352125406265259 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}